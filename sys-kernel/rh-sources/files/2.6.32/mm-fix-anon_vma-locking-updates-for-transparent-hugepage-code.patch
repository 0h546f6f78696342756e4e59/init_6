From: Andrea Arcangeli <aarcange@redhat.com>
Date: Sun, 21 Feb 2010 13:59:10 -0500
Subject: [mm] fix anon_vma locking updates for transparent hugepage code
Message-id: <20100221135910.GR5955@random.random>
Patchwork-id: 23369
O-Subject: Re: [RHEL6 PATCH 2/2 -v2] anon_vma locking updates for transparent
	hugepage code
Bugzilla: 564515
RH-Acked-by: Rik van Riel <riel@redhat.com>

I see no need to touch khugepaged (like the acked patch does), but
instead I see a need to alter split_huge_page (the acked patch didn't
do it).

The reason is that collapse_huge_page currently only works on
not-shared anon pages (so vma->anon_vma->lock is enough) while
split_huge_page (as shown by the fact it is walking the
anon_vma_chain) has to take the lock on the right anon_vma.

So I think this should be more appropriate patch and it avoids the
need of taking all locks (I hope). I tested it a bit with -mm but not
yet on rhel6.

-------

Find the anon_vma to lock from the page rather than from the vma, after recent
anon_vma changes that allows a vma to belong to more than a single anon_vma.

On Fri, Feb 19, 2010 at 01:47:49PM -0500, Rik van Riel wrote:
> Please add a comment to collapse_huge_pages that indicates why
> it is safe to lock just that one anon_vma.
>
> This code is too subtle to leave without a comment.

btw, I find not very comfortable to work with git and not with a quilt
patchset, but I'll go incremental....

Here the updated patch that also updates other comments. Please ACK
and apply to rhel6.

-----------
Subject: page anon_vma

Find the anon_vma to lock from the page rather than from the vma, after recent
anon_vma changes that allows a vma to belong to more than a single anon_vma.

Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>

diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 25a9d85..9ee8f9f 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1107,29 +1107,29 @@ static void __split_huge_page(struct page *page,
 void __split_huge_page_vma(struct vm_area_struct *vma, pmd_t *pmd)
 {
 	struct page *page;
-	struct anon_vma *anon_vma;
 	struct mm_struct *mm;
 
 	BUG_ON(vma->vm_flags & VM_HUGETLB);
 
 	mm = vma->vm_mm;
 
-	anon_vma = vma->anon_vma;
-
-	spin_lock(&anon_vma->lock);
-	BUG_ON(pmd_trans_splitting(*pmd));
 	spin_lock(&mm->page_table_lock);
 	if (unlikely(!pmd_trans_huge(*pmd))) {
 		spin_unlock(&mm->page_table_lock);
-		spin_unlock(&anon_vma->lock);
 		return;
 	}
 	page = pmd_page(*pmd);
+	VM_BUG_ON(!page_count(page));
+	get_page(page);
 	spin_unlock(&mm->page_table_lock);
 
-	__split_huge_page(page, anon_vma);
+	/*
+	 * The vma->anon_vma->lock is the wrong lock if the page is shared,
+	 * the anon_vma->lock pointed by page->mapping is the right one.
+	 */
+	split_huge_page(page);
 
-	spin_unlock(&anon_vma->lock);
+	put_page(page);
 	BUG_ON(pmd_trans_huge(*pmd));
 }
 
@@ -1361,8 +1361,8 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		/*
 		 * We can do it before isolate_lru_page because the
 		 * page can't be freed from under us. NOTE: PG_lock
-		 * seems entirely unnecessary but in doubt this is
-		 * safer. If proven unnecessary it can be removed.
+		 * is needed to serialize against split_huge_page
+		 * when invoked from the VM.
 		 */
 		if (!trylock_page(page)) {
 			release_pte_pages(pte, _pte);
@@ -1418,29 +1418,6 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 	}
 }
 
-/*
- * This cannot lead to a deadlock because the chains of anon_vmas
- * are always in the order "self, parent, grandparent".  No two
- * processes can have anon_vmas in inverted order in their chains.
- */
-static void lock_anon_vmas(struct vm_area_struct *vma)
-{
-	struct anon_vma_chain *avc;
-
-	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma) {
-		spin_lock_nest_lock(&avc->anon_vma->lock, &vma->vm_mm->mmap_sem);
-	}
-}
-
-static void unlock_anon_vmas(struct vm_area_struct *vma)
-{
-	struct anon_vma_chain *avc;
-
-	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma) {
-		spin_unlock(&avc->anon_vma->lock);
-	}
-}
-
 static void collapse_huge_page(struct mm_struct *mm,
 			       unsigned long address,
 			       struct page **hpage)
@@ -1495,8 +1472,14 @@ static void collapse_huge_page(struct mm_struct *mm,
 	if (!pmd_present(*pmd) || pmd_trans_huge(*pmd))
 		goto out;
 
-	/* stop anon_vma rmap pagetable access */
-	lock_anon_vmas(vma);
+	/*
+	 * Stop anon_vma rmap pagetable access. vma->anon_vma->lock is
+	 * enough for now (we don't need to check each anon_vma
+	 * pointed by each page->mapping) because collapse_huge_page
+	 * only works on not-shared anon pages (that are guaranteed to
+	 * belong to vma->anon_vma).
+	 */
+	spin_lock(&vma->anon_vma->lock);
 
 	pte = pte_offset_map(pmd, address);
 	ptl = pte_lockptr(mm, pmd);
@@ -1516,7 +1499,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 		BUG_ON(!pmd_none(*pmd));
 		set_pmd_at(mm, address, pmd, _pmd);
 		spin_unlock(&mm->page_table_lock);
-		unlock_anon_vmas(vma);
+		spin_unlock(&vma->anon_vma->lock);
 		goto out;
 	}
 
@@ -1524,7 +1507,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * All pages are isolated and locked so anon_vma rmap
 	 * can't run anymore.
 	 */
-	unlock_anon_vmas(vma);
+	spin_unlock(&vma->anon_vma->lock);
 
 	new_page = *hpage;
 	__collapse_huge_page_copy(pte, new_page, vma, address, ptl);
