From: George Beshers <gbeshers@redhat.com>
Date: Mon, 18 Jan 2010 22:19:24 -0500
Subject: [uv] x86: Remove move_cleanup_count from irq_cfg
Message-id: <20100118221103.4162.78914.sendpatchset@dhcp-100-2-194.bos.redhat.com>
Patchwork-id: 22641
O-Subject: [RHEL6 PATCH 5/7] x86, Remove move_cleanup_count from irq_cfg
Bugzilla: 546668
RH-Acked-by: Prarit Bhargava <prarit@redhat.com>
RH-Acked-by: Dean Nelson <dnelson@redhat.com>

Minor fuzz corrections for rhel6-5 otherwise this is patch:

commit 23359a88e7eca3c4f402562b102f23014db3c2aa
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Oct 26 14:24:33 2009 -0800

    x86: Remove move_cleanup_count from irq_cfg

    move_cleanup_count for each irq in irq_cfg is keeping track of
    the total number of cpus that need to free the corresponding
    vectors associated with the irq which has now been migrated to
    new destination. As long as this move_cleanup_count is non-zero
    (i.e., as long as we have n't freed the vector allocations on
    the old destinations) we were preventing the irq's further
    migration.

    This cleanup count is unnecessary and it is enough to not allow
    the irq migration till we send the cleanup vector to the
    previous irq destination, for which we already have irq_cfg's
    move_in_progress.  All we need to make sure is that we free the
    vector at the old desintation but we don't need to wait till
    that gets freed.

    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Gary Hade <garyhade@us.ibm.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    LKML-Reference: <20091026230001.752968906@sbs-t61.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

Signed-off-by: Aristeu Rozanski <arozansk@redhat.com>

diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h
index 9ca6ec0..004de62 100644
--- a/arch/x86/include/asm/hw_irq.h
+++ b/arch/x86/include/asm/hw_irq.h
@@ -94,7 +94,6 @@ struct irq_cfg {
 	struct irq_pin_list	*irq_2_pin;
 	cpumask_var_t		domain;
 	cpumask_var_t		old_domain;
-	unsigned		move_cleanup_count;
 	u8			vector;
 	u8			move_in_progress : 1;
 };
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index 1e82dfe..4679c98 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -1161,7 +1161,7 @@ __assign_irq_vector(int irq, struct irq_cfg *cfg, const struct cpumask *mask)
 	int cpu, err;
 	cpumask_var_t tmp_mask;
 
-	if ((cfg->move_in_progress) || cfg->move_cleanup_count)
+	if (cfg->move_in_progress)
 		return -EBUSY;
 
 	if (!alloc_cpumask_var(&tmp_mask, GFP_ATOMIC))
@@ -2217,14 +2217,10 @@ void send_cleanup_vector(struct irq_cfg *cfg)
 
 	if (unlikely(!alloc_cpumask_var(&cleanup_mask, GFP_ATOMIC))) {
 		unsigned int i;
-		cfg->move_cleanup_count = 0;
-		for_each_cpu_and(i, cfg->old_domain, cpu_online_mask)
-			cfg->move_cleanup_count++;
 		for_each_cpu_and(i, cfg->old_domain, cpu_online_mask)
 			apic->send_IPI_mask(cpumask_of(i), IRQ_MOVE_CLEANUP_VECTOR);
 	} else {
 		cpumask_and(cleanup_mask, cfg->old_domain, cpu_online_mask);
-		cfg->move_cleanup_count = cpumask_weight(cleanup_mask);
 		apic->send_IPI_mask(cleanup_mask, IRQ_MOVE_CLEANUP_VECTOR);
 		free_cpumask_var(cleanup_mask);
 	}
@@ -2414,8 +2410,6 @@ asmlinkage void smp_irq_move_cleanup_interrupt(void)
 
 		cfg = irq_cfg(irq);
 		spin_lock(&desc->lock);
-		if (!cfg->move_cleanup_count)
-			goto unlock;
 
 		if (vector == cfg->vector && cpumask_test_cpu(me, cfg->domain))
 			goto unlock;
@@ -2433,7 +2427,6 @@ asmlinkage void smp_irq_move_cleanup_interrupt(void)
 			goto unlock;
 		}
 		__get_cpu_var(vector_irq)[vector] = -1;
-		cfg->move_cleanup_count--;
 unlock:
 		spin_unlock(&desc->lock);
 	}
